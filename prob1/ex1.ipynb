{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2242 2242\n"
     ]
    }
   ],
   "source": [
    "infile = 'ca01'\n",
    "with open(infile,'rb') as f:\n",
    "#     dialect=csv.Sniffer().sniff(f.read(1024),delimiters=\" ,;\")\n",
    "    words = []\n",
    "    tags = []\n",
    "    reader=csv.reader(f,delimiter = \"\\n\")\n",
    "    for row in reader:\n",
    "        if len(row)>0:\n",
    "            words_tagged = row[0].split()\n",
    "            words.extend([re.split(r'/',elem)[0] for elem in words_tagged])\n",
    "            tags.extend([re.split(r'/',elem)[1] for elem in words_tagged])\n",
    "    print len(words),len(tags)\n",
    "    dict = sorted(set(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import numpy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem.porter import *\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "data_path = \"/home/debojyoti/brown/\"\n",
    "files=[os.path.join(data_path,f) for f in os.listdir(data_path) if re.match(r'c.+',f)!=None]\n",
    "vocab = set([])\n",
    "# stemmer = PorterStemmer()\n",
    "# stemmer = SnowballStemmer('english')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "print wordnet_lemmatizer.lemmatize('said',pos='v')\n",
    "files=[\"ca01\"]\n",
    "for infile in files:\n",
    "    with open(infile) as myfile:\n",
    "        data_orig=myfile.read()\n",
    "        data = re.sub(r\"(\\S+)/(\\S+)\",r\"\\1\",data_orig,flags=re.DOTALL)\n",
    "        labels = re.sub(r\"(\\S+)/(\\S+)\",r\"\\2\",data_orig,flags=re.DOTALL)\n",
    "        # whitespace removal and Stemming\n",
    "#         data = \" \".join(data.split())\n",
    "#         data = \" \".join([stemmer.stem(elem) for elem in data.split()])\n",
    "        lst=[]\n",
    "        for elem,label in zip(data.split(),labels.split()):\n",
    "            if re.match(r'vb.*',label)!=None:\n",
    "                lst.append(wordnet_lemmatizer.lemmatize(elem,pos='v'))\n",
    "            elif re.match(r'nn.*',label)!=None:\n",
    "                lst.append(wordnet_lemmatizer.lemmatize(elem,pos='n'))\n",
    "            else:\n",
    "                lst.append(elem)\n",
    "        lst=[str(i) for i in lst]\n",
    "#         print lst\n",
    "        data = \" \".join(lst)\n",
    "#         print data\n",
    "        # Replace number with pseudowords\n",
    "        data = re.sub(r\"\\d+(.\\d+)?\",\"NUM\",data)\n",
    "        # Tokenize\n",
    "        tokens = nltk.word_tokenize(data)\n",
    "        # Build vocabulary\n",
    "        vocab.update(set(tokens))\n",
    "vocabulary = numpy.asarray(sorted(vocab))\n",
    "indices = numpy.arange(vocabulary.size)\n",
    "word2indx = {}\n",
    "for key,val in zip(vocabulary,indices):\n",
    "    word2indx[key]=val\n",
    "sentences = sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Softmax Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano,numpy\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_in = 10\n",
    "n_out = 5\n",
    "n_samples = 12\n",
    "# Initialize weight matrix\n",
    "W = theano.shared(\n",
    "    value = numpy.ones(\n",
    "        (n_in,n_out),\n",
    "        dtype = theano.config.floatX\n",
    "    ),\n",
    "    name = 'W',\n",
    "    borrow = True\n",
    ")\n",
    "# print W.eval()\n",
    "# Initialize bias\n",
    "b = theano.shared(\n",
    "    value = numpy.ones(\n",
    "        (n_out,),\n",
    "        dtype = theano.config.floatX\n",
    "    ),\n",
    "    name = 'b',\n",
    "    borrow = True\n",
    ")\n",
    "X=numpy.random.rand(n_samples,n_in)\n",
    "p_y_given_x = T.nnet.softmax(T.dot(X,W) + b)\n",
    "# p_y_given_x.eval()\n",
    "y_pred = T.argmax(p_y_given_x, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function: Negative Log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6094379124340998"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=numpy.random.randint(5,size=n_samples)\n",
    "-T.mean(T.log(p_y_given_x)[T.arange(y.size),y]).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self, inpt, n_in, n_out):\n",
    "        self.W = theano.shared(\n",
    "            value = numpy.zeros(\n",
    "                (n_in,n_out),\n",
    "                dtype = theano.config.floatX\n",
    "            ),\n",
    "            name = 'W',\n",
    "            borrow = True\n",
    "        )\n",
    "        self.b = theano.shared(\n",
    "            value = numpy.zeros(\n",
    "                (n_out,),\n",
    "                dtype = theano.config.floatX\n",
    "            ),\n",
    "            name = 'b',\n",
    "            borrow = True\n",
    "        )\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(inpt,self.W) + self.b)\n",
    "        self.y_pred = T.argmax(p_y_given_x, axis=1)\n",
    "        self.params = [self.W,self.b]\n",
    "        self.inpt = inpt\n",
    "    \n",
    "    def negative_log_likelihood(self,y):\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.size),y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate symbolic variables for input and labels\n",
    "x = T.matrix('x')\n",
    "y = T.ivector('y')\n",
    "classifier = LogisticRegression(inpt=x, n_in=20*20, n_out=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# handle for cost\n",
    "cost = classifier.negative_log_likelihood(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gradients w.r.t W and b\n",
    "g_W=T.grad(cost=cost,wrt=classifier.W)\n",
    "g_b=T.grad(cost=cost,wrt=classifier.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Update Eqn\n",
    "learning_rate=T.dscalar('learning_rate')\n",
    "updates = [(classifier.W,classifier.W-learning_rate*g_W),\n",
    "           (classifier.b,classifier.b-learning_rate*g_b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training Model\n",
    "# index=T.iscalar('index')\n",
    "# train_set_x=T.matrix('train_set')\n",
    "# train_set_y=T.dvector('labels')\n",
    "# batch_size=T.iscalar('batch_size')\n",
    "# train_model = theano.function(\n",
    "#     inputs=[index],\n",
    "#     outputs=cost,\n",
    "#     updates=updates,\n",
    "#     givens={\n",
    "#         x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "#         y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "mygenerator = (x*x for x in range(3))\n",
    "for i in mygenerator:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    }
   ],
   "source": [
    "def example():\n",
    "    for i in range(5):\n",
    "        yield i\n",
    "ex=example()\n",
    "x=next(ex)\n",
    "y=next(ex)\n",
    "print x,y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
