{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy\n",
    "import datetime\n",
    "import nltk\n",
    "from subprocess import Popen,PIPE\n",
    "# To print all outputs, not just the last one\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most frequent tagging , NLTK tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BrownCorpus(object):\n",
    "    def __init__(self,dirname):\n",
    "        self.dirname = dirname\n",
    "        self.pos_ct = {}\n",
    "        self.final_pos = {}\n",
    "        self.final_ct = {}\n",
    "        total = len(os.listdir(self.dirname))\n",
    "        self.train_size = int(total*0.8)\n",
    "        self.test_size = total-self.train_size\n",
    "    def compute_pos_ct(self):\n",
    "        for indx,fname in enumerate(os.listdir(self.dirname)):\n",
    "            # Run on training set\n",
    "            if indx>=self.train_size:\n",
    "                break\n",
    "            if re.match(r'c.+',fname)==None:\n",
    "                continue\n",
    "            fname = os.path.join(self.dirname,fname)\n",
    "            if not os.path.isfile(fname):\n",
    "                continue\n",
    "            for line in open(fname):\n",
    "                token_tags = [t.split('/') for t in line.split() if len(t.split('/'))==2]\n",
    "                for token,tag in token_tags:\n",
    "                    token = str(token).lower()\n",
    "                    try:\n",
    "                        self.pos_ct[(token,tag)] +=1\n",
    "                    except:\n",
    "                        self.pos_ct[(token,tag)] = 1\n",
    "    def build_pos_oracle(self):\n",
    "        for token_tag in self.pos_ct:\n",
    "            ct = self.pos_ct[token_tag]\n",
    "            token = token_tag[0]\n",
    "            tag = token_tag[1]\n",
    "            try:\n",
    "                if self.final_ct[token]<ct:\n",
    "                    self.final_pos[token]=tag\n",
    "                    self.final_ct[token]=ct\n",
    "            except:\n",
    "                self.final_pos[token]=tag\n",
    "                self.final_ct[token]=ct\n",
    "    def get_accuracy(self):\n",
    "        total_tokens = 0\n",
    "        error_ct = 0\n",
    "        for indx,fname in enumerate(os.listdir(self.dirname)):\n",
    "            if indx<self.train_size:\n",
    "                continue\n",
    "            if re.match(r'c.+',fname)==None:\n",
    "                continue\n",
    "            fname = os.path.join(self.dirname,fname)\n",
    "            if not os.path.isfile(fname):\n",
    "                continue\n",
    "            for line in open(fname):\n",
    "                token_tags = [t.split('/') for t in line.split() if len(t.split('/'))==2]\n",
    "                total_tokens += len(token_tags)\n",
    "                for token,tag in token_tags:\n",
    "                    token = str(token).lower()\n",
    "                    try:\n",
    "                        if tag!=self.final_pos[token]:\n",
    "                            error_ct+=1\n",
    "                    except:\n",
    "                        pass\n",
    "        return 1-error_ct/float(total_tokens)\n",
    "    def nltk_tagger_accuracy(self):\n",
    "        total_tokens = 0\n",
    "        error_ct = 0\n",
    "        for indx,fname in enumerate(os.listdir(self.dirname)):\n",
    "            if indx<self.train_size:\n",
    "                continue\n",
    "            if re.match(r'c.+',fname)==None:\n",
    "                continue\n",
    "            fname = os.path.join(self.dirname,fname)\n",
    "            if not os.path.isfile(fname):\n",
    "                continue\n",
    "            for line in open(fname):\n",
    "                token_tags = [nltk.tag.str2tuple(t) for t in line.split() if len(t.split('/'))==2]\n",
    "                total_tokens += len(token_tags)\n",
    "                tokens = [tt[0] for tt in token_tags]\n",
    "                tokens_txt = \" \".join(tokens)\n",
    "                nltk_tags = nltk.pos_tag(tokens_txt)\n",
    "                for i in range(len(token_tags)):\n",
    "                    nltk_tag = nltk_tags[i][1]\n",
    "                    given_tag = token_tags[i][1]\n",
    "                    if nltk_tag.lower()[:2] != given_tag.lower()[:2]:\n",
    "                        error_ct += 1\n",
    "        return 1-error_ct/float(total_tokens)\n",
    "    def opennlp_tagger_accuracy(self):\n",
    "        total_tokens = 0\n",
    "        error_ct = 0\n",
    "        outfile = open('opennlp_tag','w')\n",
    "        doc = \"\"\n",
    "        for indx,fname in enumerate(os.listdir(self.dirname)):\n",
    "            if indx<self.train_size:\n",
    "                continue\n",
    "            if re.match(r'c.+',fname)==None:\n",
    "                continue\n",
    "            fname = os.path.join(self.dirname,fname)\n",
    "            if not os.path.isfile(fname):\n",
    "                continue\n",
    "            for line in open(fname):\n",
    "                token_tags = [nltk.tag.str2tuple(t) for t in line.split() if len(t.split('/'))==2]\n",
    "                total_tokens += len(token_tags)\n",
    "                tokens = [tt[0] for tt in token_tags]\n",
    "                tokens_txt = \" \".join(tokens)\n",
    "                doc += tokens_txt\n",
    "                doc += \"\\n\"\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-16 21:05:47.238152\n",
      "2016-10-16 21:05:48.028085\n",
      "2016-10-16 21:05:48.075190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8422427445175061"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown = BrownCorpus('/home/debojyoti/brown/')\n",
    "brown.compute_pos_ct()\n",
    "brown.build_pos_oracle()\n",
    "brown.get_accuracy()\n",
    "brown.nltk_tagger_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "a=[1,2,3]\n",
    "a.extend(a)\n",
    "print a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenNLP tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The_DT Fulton_NNP \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "T\n",
      "T\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-0e5b7b1ed804>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from subprocess import Popen,PIPE,STDOUT\n",
    "# os.system('../apache-opennlp-1.6.0/bin/opennlp POSTagger en-pos-perceptron.bin < /home/debojyoti/brown/ca01')\n",
    "doc = \"\"\n",
    "tags_all = []\n",
    "total_tokens = 0\n",
    "err_ct = 0\n",
    "with open('/home/debojyoti/brown/ca01') as infile:\n",
    "    for line in infile:\n",
    "        token_tags = [nltk.tag.str2tuple(t) for t in line.split() if len(t.split('/'))==2]\n",
    "        tokens = [tt[0] for tt in token_tags]\n",
    "        tags = [tt[1] for tt in token_tags]\n",
    "        tags_all.extend(tags)\n",
    "        tokens_txt = \" \".join(tokens)\n",
    "        doc += tokens_txt\n",
    "        doc += \"\\n\"    \n",
    "    cmd = ['../apache-opennlp-1.6.0/bin/opennlp', 'POSTagger', '../apache-opennlp-1.6.0/bin/en-pos-perceptron.bin']\n",
    "    myprogram = Popen(cmd,stdin=PIPE,stdout=PIPE,stderr=STDOUT)\n",
    "    stdout = myprogram.communicate(input=doc)[0]\n",
    "    stdout = stdout.split('\\n',1)[1]\n",
    "    print stdout[:20]\n",
    "    for line in stdout:\n",
    "        try:\n",
    "            print line\n",
    "            tags = [t.split('_')[1] for t in line.split()]\n",
    "        except:\n",
    "            print line\n",
    "            raise\n",
    "        total_tokens += len(tags)\n",
    "        for indx,tag in enumerate(tags):\n",
    "            if tag.lower()!=tags_all[i]:\n",
    "                err_ct += 1\n",
    "print total_tokens,err_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcabc'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=\"abc\"\n",
    "x+x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
